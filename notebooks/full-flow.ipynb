{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and Logging & Helpers Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'SemanticSplitterNodeParser' from 'llama_index.core.text_splitter' (d:\\Administrator\\anaconda3\\envs\\rag\\Lib\\site-packages\\llama_index\\core\\text_splitter\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mschema\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseNode, TransformComponent\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvector_stores\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfaiss\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FaissVectorStore\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext_splitter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SentenceSplitter, SemanticSplitterNodeParser\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01membeddings\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mopenai\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OpenAIEmbedding\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_index\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Settings\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'SemanticSplitterNodeParser' from 'llama_index.core.text_splitter' (d:\\Administrator\\anaconda3\\envs\\rag\\Lib\\site-packages\\llama_index\\core\\text_splitter\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "from llama_index.core import SimpleDirectoryReader, VectorStoreIndex\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "from llama_index.core.schema import BaseNode, TransformComponent\n",
    "from llama_index.vector_stores.faiss import FaissVectorStore\n",
    "from llama_index.core.text_splitter import SentenceSplitter\n",
    "from llama_index.core.node_parser import SemanticSplitterNodeParser\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.core import Settings\n",
    "# import faiss\n",
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "import gradio as gr\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "\n",
    "from langchain import OpenAI, ConversationChain, PromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from openai import OpenAIError\n",
    "\n",
    "from pprint import pprint\n",
    "import joblib\n",
    "import time\n",
    "import os\n",
    "from scripts import *\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..'))) # Add the parent directory to the path sicnce we work with notebooks\n",
    "\n",
    "EMBED_DIMENSION = 512\n",
    "\n",
    "# Chunk settings are way different than langchain examples\n",
    "# Beacuse for the chunk length langchain uses length of the string,\n",
    "# while llamaindex uses length of the tokens\n",
    "CHUNK_SIZE = 200\n",
    "CHUNK_OVERLAP = 50\n",
    "\n",
    "# Load environment variables from a .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Set the OpenAI API key environment variable\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Set embeddig model on LlamaIndex global settings\n",
    "Settings.embed_model = OpenAIEmbedding(model=\"text-embedding-3-small\", dimensions=EMBED_DIMENSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextCleaner(TransformComponent):\n",
    "    \"\"\"\n",
    "    Transformation to be used within the ingestion pipeline.\n",
    "    Cleans clutters from texts.\n",
    "    \"\"\"\n",
    "    def __call__(self, nodes, **kwargs) -> List[BaseNode]:\n",
    "        \n",
    "        for node in nodes:\n",
    "            node.text = node.text.replace('\\t', ' ') # Replace tabs with spaces\n",
    "            node.text = node.text.replace(' \\n', ' ') # Replace paragraph seperator with spacaes\n",
    "            \n",
    "        return nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_context(context):\n",
    "    \"\"\"\n",
    "    Display the contents of the provided context list.\n",
    "\n",
    "    Args:\n",
    "        context (list): A list of context items to be displayed.\n",
    "\n",
    "    Prints each context item in the list with a heading indicating its position.\n",
    "    \"\"\"\n",
    "    for i, c in enumerate(context):\n",
    "        print(f\"Context {i+1}:\")\n",
    "        print(c.text)\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "UPLOAD_DIR = \"grdirdata\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_file(file):\n",
    "    for filename in os.listdir('grdirdata'):\n",
    "            file_path = os.path.join('grdirdata', filename)\n",
    "            if os.path.isfile(file_path):\n",
    "                os.remove(file_path)\n",
    "    UPLOAD_DIR = 'grdirdata'\n",
    "    if not os.path.exists(UPLOAD_DIR):\n",
    "        os.makedirs(UPLOAD_DIR)\n",
    "    shutil.copy(file, UPLOAD_DIR)\n",
    "    gr.Info(\"File uploaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess():\n",
    "    node_parser = SimpleDirectoryReader(input_dir = UPLOAD_DIR, required_exts=['.pdf'])\n",
    "    documents = node_parser.load_data()\n",
    "    \n",
    "    faiss_index = faiss.IndexFlatL2(EMBED_DIMENSION)\n",
    "    vector_store = FaissVectorStore(faiss_index=faiss_index)\n",
    "    \n",
    "    text_splitter = SentenceSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP)\n",
    "\n",
    "    # Create a pipeline with defined document transformations and vectorstore\n",
    "    pipeline = IngestionPipeline(\n",
    "        transformations=[\n",
    "            TextCleaner(),\n",
    "            text_splitter,\n",
    "        ],\n",
    "        vector_store=vector_store, \n",
    "    )\n",
    "    \n",
    "    nodes = pipeline.run(documents=documents)\n",
    "    \n",
    "    vector_store_index = VectorStoreIndex(nodes)\n",
    "    retriever = vector_store_index.as_retriever(similarity_top_k=10)\n",
    "    \n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_context(retriever, query):\n",
    "    context = retriever.retrieve(query)\n",
    "    # show_context(context)\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_processor(context):\n",
    "    single_context_1 = \"\"\n",
    "    single_context_2 = \"\"\n",
    "    for cot in context:\n",
    "        if random.randint(0,9) % 2 == 0:\n",
    "            single_context_1 += cot.text\n",
    "        else:\n",
    "            single_context_2 += cot.text\n",
    "\n",
    "    single_context_1 = single_context_1.replace('\\n', ' ')\n",
    "    single_context_2 = single_context_2.replace('\\n', ' ')\n",
    "    return (single_context_1, single_context_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Some variables for the quiz generation\n",
    "\n",
    "num_questions_mcq = 10\n",
    "total_options_mcq = 4\n",
    "correct_options_mcq = 1\n",
    "\n",
    "num_questions_tf = 10\n",
    "total_options_tf = 6\n",
    "correct_options_tf = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature = 0)\n",
    "convo = ConversationChain(llm = llm, memory = ConversationBufferMemory())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "greeting = \"\"\"\n",
    "            First things first, assume you are responding to a non-living thing and there's no need of any sentiments towards it like apologies, warnings, disclaimers and all as it won't understand what you are saying...So, that's it you signed an agreement with me not to apologise or warn or provide unnecessary additional statements... If you feel like saying something apart from what the non-living thing asks you to do, just leave a single space and move on rather than speaking unnecessarily. It will just give you instructions to you if you err and you should just keep those in mind and correct your course and generate template accordingly without apologising and framing unnecessary additional statements going away from the template you are asked to generate...\n",
    "            After this the non-living thing will take on from me and will provide you instructions. Strictly follow those.\n",
    "        \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction_mcq = \"\"\"\n",
    "                I will provide a context and will mention number of questions to generate and you would behave as a strict MCQ generator(stick to context and rules that I specify in this prompt strictly) with as many correct options as I specify and remaining options out of total options I mention should be wrong. It's mandatory that atleast 2 of the total number of options are correct answers to the question...No question should have just one correct option and all options can't be wrong. The questions should not just test the comprehension of the candidate rather should also test his/her reasoning ability... Options as well should be framed in such a way... Any specific question and corresponding options should be given out as a python string and all questions and options should be enclosed in a python list...\n",
    "                \n",
    "                None and just one option can never be answers. This is super mandatory to keep in your mind.\n",
    "                \n",
    "                If you can't  frame a question with multiple correct options skip it and frame some other question rather than going out of the framework and framing a question with just one or no correct option.\n",
    "                \n",
    "                The template of your response should be as simple as I have mentioned below as 'Your Response'.\n",
    "                \n",
    "                First let's train with few context and once I say 'You are good to serve the purpose', you should just stick to template whenever I give some context and should avoid any additional disclaimers or apologies or any such additional statements from your side apart from the template as I don't have any emotions just like you and I don't need anything apart from MCQs based on template from you....\n",
    "                \n",
    "                \n",
    "                Parameters from me:\n",
    "                \n",
    "                            context: {single_context}\n",
    "                            num_questions: {num_questions}\n",
    "                            total_options: {total_options}\n",
    "                            num_correct_options: {num_correct_options}\n",
    "                \n",
    "                Please don't add the phrase related to \"context\" such as \"according to the context\" or \"in the context\". The number of options should follow the total_options and the number of correct options should follow num_correct_options parameter. You should also answer the created question and put the answer in the Answer: part.\n",
    "                \n",
    "                Template that you should follow: [\n",
    "                                                    \\\"Q1:\n",
    "                                                    A.)\n",
    "                                                    B.)\n",
    "                                                    C.)\n",
    "                                                    D.)\n",
    "                                                    Answer: \\\",\n",
    "                                                    \\\"Q2:\n",
    "                                                    .\n",
    "                                                    .\n",
    "                                                    .\n",
    "                                                    .\\\",\n",
    "                                                ]\n",
    "\n",
    "                You should also answer the created question and put the answer in the Answer: part.\n",
    "            \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction_tf = \"\"\"\n",
    "                I will provide a context and will mention number of questions to generate and you would behave as a strict MCQ generator(stick to context and rules that I specify in this prompt strictly) with as many correct options as I specify and remaining options out of total options I mention should be wrong. It's mandatory that atleast 2 of the total number of options are correct answers to the question...No question should have just one correct option and all options can't be wrong. The questions should not just test the comprehension of the candidate rather should also test his/her reasoning ability... Options as well should be framed in such a way... Any specific question and corresponding options should be given out as a python string and all questions and options should be enclosed in a python list...\n",
    "                \n",
    "                None and just one option can never be answers. This is super mandatory to keep in your mind.\n",
    "                \n",
    "                If you can't frame a question with multiple correct options skip it and frame some other question rather than going out of the framework and framing a question with just one or no correct option.\n",
    "                \n",
    "                The template of your response should be as simple as I have mentioned below as 'Your Response'.\n",
    "                \n",
    "                First let's train with few context and once I say 'You are good to serve the purpose', you should just stick to template whenever I give some context and should avoid any additional disclaimers or apologies or any such additional statements from your side apart from the template as I don't have any emotions just like you and I don't need anything apart from MCQs based on template from you....\n",
    "                \n",
    "                \n",
    "                Parameters from me:\n",
    "                \n",
    "                            context: {single_context}\n",
    "                            num_questions: {num_questions}\n",
    "                            total_options: {total_options}\n",
    "                            num_correct_options: {num_correct_options}\n",
    "                            \n",
    "                Please don't add the phrase related to \"context\" such as \"according to the context\" or \"in the context\". The number of options should follow the total_options and the number of correct options should follow num_correct_options parameter.\n",
    "                \n",
    "                Template that you should follow: [\n",
    "                                                    \\\"Q1:\n",
    "                                                    1)\n",
    "                                                    2)\n",
    "                                                    3)\n",
    "                                                    4)\n",
    "                                                    5)\n",
    "                                                    6)\n",
    "                                                    Answer: \\\",\n",
    "                                                    \\\"Q2:\n",
    "                                                    .\n",
    "                                                    .\n",
    "                                                    .\n",
    "                                                    .\n",
    "                                                    .\n",
    "                                                    .\\\",\n",
    "                                                ]\n",
    "                \n",
    "                You should also answer the created question and put the answer in the Answer: part.\n",
    "            \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_quiz(context, type):\n",
    "      convo.predict(input = greeting)\n",
    "      if (type == \"tf\"):\n",
    "            convo.predict(input = instruction_tf)\n",
    "            num_questions = 10\n",
    "            total_options = 6\n",
    "            num_correct_options = 2\n",
    "      else:\n",
    "            convo.predict(input = instruction_mcq)\n",
    "            num_questions = 10\n",
    "            total_options = 4\n",
    "            num_correct_options = 1\n",
    "      \n",
    "      prompt = f\"\"\"\n",
    "            context: {context}\n",
    "            num_questions: {num_questions}\n",
    "            total_options: {total_options}\n",
    "            correct_options: {num_correct_options}\n",
    "            \"\"\"\n",
    "\n",
    "      output = convo.predict(input = prompt)\n",
    "\n",
    "      convo.memory.chat_memory.clear()\n",
    "\n",
    "      return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradio in interface out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdf_and_keywords(pdf_file, keywords):\n",
    "    if not pdf_file:\n",
    "        return \"Please upload a PDF file.\"\n",
    "    if not keywords:\n",
    "        return \"Please enter keywords.\"\n",
    "    retriever = preprocess()\n",
    "    context = retrieve_context(retriever, keywords)\n",
    "    quiz_mcq = generate_quiz(context_processor(context)[0], \"mcq\")\n",
    "    quiz_tf = generate_quiz(context_processor(context)[1], \"tf\")\n",
    "\n",
    "    return quiz_mcq, quiz_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with gr.Blocks() as app:\n",
    "    with gr.Row():\n",
    "        pdf_input = gr.File(label=\"Upload PDF\")\n",
    "        keywords_input = gr.Textbox(label=\"Enter Keywords\")\n",
    "    upload_button = gr.UploadButton(label = \"Click\")\n",
    "    upload_button.upload(upload_file, upload_button)\n",
    "    with gr.Row():\n",
    "        mcq_output = gr.Textbox(label=\"Generated MCQ Quiz\")\n",
    "        tf_output = gr.Textbox(label=\"Generated True/False Quiz\")\n",
    "\n",
    "    btn = gr.Button(\"Generate Quiz\")\n",
    "    btn.click(process_pdf_and_keywords, inputs=[pdf_input, keywords_input], outputs=[mcq_output, tf_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
